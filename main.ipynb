{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af916d07",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11564\\4074906705.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models.backbone import ResNet_18\n",
    "import dataset.raf as dataset\n",
    "from losses import SupConLoss\n",
    "from utils import Bar, Logger, AverageMeter, accuracy, mkdir_p, savefig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96771a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='PyTorch MixMatch Training')\n",
    "# Optimization options\n",
    "parser.add_argument('--epochs', default=2, type=int, metavar='N',\n",
    "                    help='number of total epochs to run')\n",
    "parser.add_argument('--start-epoch', default=1, type=int, metavar='N',\n",
    "                    help='manual epoch number (useful on restarts)')\n",
    "parser.add_argument('--batch-size', default=16, type=int, metavar='N',\n",
    "                    help='train batchsize')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.0005, type=float,\n",
    "                    metavar='LR', help='initial learning rate')\n",
    "parser.add_argument('--num_workers', type=int, default=1,\n",
    "                        help='num of workers to use')\n",
    "# Checkpoints\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH',\n",
    "                    help='path to latest checkpoint (default: none)')\n",
    "# Miscs\n",
    "parser.add_argument('--manualSeed', type=int, default=5, help='manual seed')\n",
    "#Device options\n",
    "parser.add_argument('--gpu', default='0', type=str,\n",
    "                    help='id(s) for CUDA_VISIBLE_DEVICES')\n",
    "#Method options\n",
    "parser.add_argument('--n-labeled', type=int, default=200,\n",
    "                        help='Number of labeled data')\n",
    "parser.add_argument('--train-iteration', type=int, default=100,\n",
    "                        help='Number of iteration per epoch')\n",
    "parser.add_argument('--out', default='result',\n",
    "                        help='Directory to output the result')\n",
    "parser.add_argument('--ema-decay', default=0.999, type=float)\n",
    "#Data\n",
    "parser.add_argument('--train-root', type=str, default='/home/gpu/FER/datasets/RAFdataset/train',\n",
    "                        help=\"root path to train data directory\")\n",
    "parser.add_argument('--test-root', type=str, default='/home/gpu/FER/datasets/RAFdataset/test',\n",
    "                        help=\"root path to test data directory\")\n",
    "parser.add_argument('--label-train', default='/home/gpu/FER/datasets/RAFdataset/RAF_train_label2.txt', type=str, help='')\n",
    "parser.add_argument('--label-test', default='/home/gpu/FER/datasets/RAFdataset/RAF_test_label2.txt', type=str, help='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abb9b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args()\n",
    "state = {k: v for k, v in args._get_kwargs()}\n",
    "\n",
    "# Use CUDA\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Random seed\n",
    "if args.manualSeed is None:\n",
    "    args.manualSeed = random.randint(1, 10000)\n",
    "np.random.seed(args.manualSeed)\n",
    "torch.manual_seed(args.manualSeed)\n",
    "torch.cuda.manual_seed_all(args.manualSeed)\n",
    "\n",
    "best_acc = 0  # best test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6462d808",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    global best_acc\n",
    "\n",
    "    if not os.path.isdir(args.out):\n",
    "        mkdir_p(args.out)\n",
    "\n",
    "    # Data\n",
    "    print(f'==> Preparing RAF-DB')\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "\n",
    "    transform_train = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.RandomApply([\n",
    "            transforms.RandomCrop(224, padding=8)\n",
    "        ], p=0.5),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ])\n",
    "\n",
    "    transform_val = transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "    ])\n",
    "\n",
    "    train_labeled_set, train_unlabeled_set, test_set = dataset.get_raf(args.train_root, args.label_train, args.test_root, args.label_test, args.n_labeled, transform_train=transform_train, transform_val=transform_val)\n",
    "\n",
    "    labeled_trainloader = data.DataLoader(train_labeled_set, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, drop_last=True)\n",
    "    unlabeled_trainloader = data.DataLoader(train_unlabeled_set, batch_size=args.batch_size, shuffle=True, num_workers=args.num_workers, drop_last=True)\n",
    "    test_loader = data.DataLoader(test_set, batch_size=64, shuffle=False, num_workers=args.num_workers)\n",
    "\n",
    "    # Model\n",
    "    print(\"==> creating ResNet-18\")\n",
    "\n",
    "    def create_model(ema=False):\n",
    "        model = ResNet_18(num_classes=7)\n",
    "        model = torch.nn.DataParallel(model).cuda()\n",
    "\n",
    "        if ema:\n",
    "            for param in model.parameters():\n",
    "                param.detach_()\n",
    "        return model\n",
    "\n",
    "    model = create_model()\n",
    "    ema_model = create_model(ema=True)\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "    print('    Total params: %.2fM' % (sum(p.numel() for p in model.parameters())/1000000.0))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    criterion_simclr = SupConLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    ema_optimizer= WeightEMA(model, ema_model, alpha=args.ema_decay)\n",
    "\n",
    "    logger = Logger(os.path.join(args.out, 'log.txt'), title='RAF')\n",
    "    logger.set_names(['Train Loss', 'Train Loss X', 'Train Loss U', 'Train Loss S', 'Test Loss', 'Test Acc.'])\n",
    "\n",
    "    test_accs = []\n",
    "    threshold = [0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8]\n",
    "    start_epoch = 1\n",
    "    # Train and val\n",
    "    for epoch in range(start_epoch, args.epochs + 1):\n",
    "\n",
    "        print('\\nEpoch: [%d | %d] LR: %f Threshold=[%.4f, %.4f, %.4f, %.4f, %.4f, %.4f, %.4f]' % (epoch, args.epochs, state['lr'], threshold[0], threshold[1], threshold[2], threshold[3], threshold[4], threshold[5], threshold[6]))\n",
    "\n",
    "        train_loss, train_loss_x, train_loss_u, train_loss_sim = train(labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_optimizer, criterion, criterion_simclr, threshold, epoch, use_cuda)\n",
    "        _, train_acc, outputs_new, targets_new = validate(labeled_trainloader, ema_model, criterion, epoch, use_cuda, mode='Train Stats')\n",
    "        threshold = adaptive_threshold_generate(outputs_new, targets_new, threshold, epoch)\n",
    "\n",
    "        test_loss, test_acc, _, _ = validate(test_loader, ema_model, criterion, epoch, use_cuda, mode='Test Stats')\n",
    "\n",
    "        # append logger file\n",
    "        logger.append([train_loss, train_loss_x, train_loss_u, train_loss_sim, test_loss, test_acc])\n",
    "\n",
    "        # save model\n",
    "        is_best = test_acc > best_acc\n",
    "        best_acc = max(test_acc, best_acc)\n",
    "        save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'ema_state_dict': ema_model.state_dict(),\n",
    "                'acc': test_acc,\n",
    "                'best_acc': best_acc,\n",
    "                'optimizer' : optimizer.state_dict(),\n",
    "            }, is_best)\n",
    "        test_accs.append(test_acc)\n",
    "    logger.close()\n",
    "\n",
    "    print('Best acc:')\n",
    "    print(best_acc)\n",
    "\n",
    "\n",
    "def train(labeled_trainloader, unlabeled_trainloader, model, optimizer, ema_optimizer, criterion_ce, criterion_simclr, threshold, epoch, use_cuda):\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    losses_x = AverageMeter()\n",
    "    losses_u = AverageMeter()\n",
    "    losses_sim = AverageMeter()\n",
    "    end = time.time()\n",
    "\n",
    "    bar = Bar('Training', max=args.train_iteration)\n",
    "    labeled_train_iter = iter(labeled_trainloader)\n",
    "    unlabeled_train_iter = iter(unlabeled_trainloader)\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx in range(args.train_iteration):\n",
    "        try:\n",
    "            inputs_x, targets_x = labeled_train_iter.next()\n",
    "        except:\n",
    "            labeled_train_iter = iter(labeled_trainloader)\n",
    "            inputs_x, targets_x = labeled_train_iter.next()\n",
    "\n",
    "        try:\n",
    "            (inputs_u, inputs_u2, inputs_strong), _ = unlabeled_train_iter.next()\n",
    "        except:\n",
    "            unlabeled_train_iter = iter(unlabeled_trainloader)\n",
    "            (inputs_u, inputs_u2, inputs_strong), _ = unlabeled_train_iter.next()\n",
    "\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        batch_size = inputs_x.size(0)\n",
    "\n",
    "\n",
    "        if use_cuda:\n",
    "            inputs_x, targets_x = inputs_x.cuda(), targets_x.cuda(non_blocking=True)\n",
    "            inputs_u = inputs_u.cuda()\n",
    "            inputs_u2 = inputs_u2.cuda()\n",
    "            inputs_strong = inputs_strong.cuda()\n",
    "\n",
    "\n",
    "        # compute guessed labels of unlabeled samples\n",
    "        outputs_u, feature_u = model(inputs_u)\n",
    "        outputs_u2, feature_u2 = model(inputs_u2)\n",
    "        p = (torch.softmax(outputs_u, dim=1) + torch.softmax(outputs_u2, dim=1)) / 2\n",
    "        max_probs, max_idx = torch.max(p, dim=1)\n",
    "        max_idx = max_idx.detach()\n",
    "\n",
    "        output_x, _ = model(inputs_x)\n",
    "\n",
    "        mask = mask_generate(max_probs, max_idx, batch_size, threshold)\n",
    "        mask_idx = np.where(mask.cpu() == 0)[0]\n",
    "        features_prob = torch.cat([feature_u[mask_idx, :].unsqueeze(1), feature_u2[mask_idx, :].unsqueeze(1)], dim=1)\n",
    "\n",
    "        Lx = criterion_ce(output_x, targets_x).mean()\n",
    "\n",
    "        if features_prob.shape[0] ==0:\n",
    "            Ls = torch.from_numpy(np.array(0))\n",
    "        else:\n",
    "            Ls = criterion_simclr(features_prob)\n",
    "\n",
    "        output_strong, _ = model(inputs_strong)\n",
    "        Lu = criterion_ce(output_strong, max_idx) * mask\n",
    "        Lu = Lu.mean()\n",
    "        loss = Lx *0.5 + Lu + Ls * 0.1\n",
    "\n",
    "        # record loss\n",
    "        losses.update(loss.item(), inputs_x.size(0))\n",
    "        losses_x.update(Lx.item(), inputs_x.size(0))\n",
    "        losses_u.update(Lu.item(), inputs_x.size(0))\n",
    "        losses_sim.update(Ls.item(), inputs_x.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ema_optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        # plot progress\n",
    "        bar.suffix  = '({batch}/{size}) Total: {total:} | Loss: {loss:.4f} | Loss_x: {loss_x:.4f} | Loss_u: {loss_u:.4f} | Loss_s: {loss_sim:.4f}'.format(\n",
    "                    batch=batch_idx + 1,\n",
    "                    size=args.train_iteration,\n",
    "                    total=bar.elapsed_td,\n",
    "                    loss=losses.avg,\n",
    "                    loss_x=losses_x.avg,\n",
    "                    loss_u=losses_u.avg,\n",
    "                    loss_sim=losses_sim.avg,\n",
    "                    )\n",
    "        bar.next()\n",
    "    bar.finish()\n",
    "\n",
    "    return (losses.avg, losses_x.avg, losses_u.avg, losses_sim.avg)\n",
    "\n",
    "def validate(valloader, model, criterion, epoch, use_cuda, mode):\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    bar = Bar(f'{mode}', max=len(valloader))\n",
    "\n",
    "    outputs_new = torch.ones(1, 7).cuda()\n",
    "    targets_new = torch.ones(1).long().cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(valloader):\n",
    "            # measure data loading time\n",
    "            data_time.update(time.time() - end)\n",
    "\n",
    "            if use_cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n",
    "            # compute output\n",
    "            outputs, _ = model(inputs)\n",
    "            loss = criterion(outputs, targets).mean()\n",
    "\n",
    "            ##\n",
    "            outputs_new = torch.cat((outputs_new, outputs), dim=0)\n",
    "            targets_new = torch.cat((targets_new, targets), dim=0)\n",
    "            ##\n",
    "\n",
    "            # measure accuracy and record loss\n",
    "            prec1, prec5 = accuracy(outputs, targets, topk=(1, 5))\n",
    "            losses.update(loss.item(), inputs.size(0))\n",
    "            top1.update(prec1.item(), inputs.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            # plot progress\n",
    "            bar.suffix  = '({batch}/{size}) Total: {total:} | Loss: {loss:.4f} | Accuracy: {top1: .4f}'.format(\n",
    "                        batch=batch_idx + 1,\n",
    "                        size=len(valloader),\n",
    "                        total=bar.elapsed_td,\n",
    "                        loss=losses.avg,\n",
    "                        top1=top1.avg,\n",
    "                        )\n",
    "            bar.next()\n",
    "        bar.finish()\n",
    "    return (losses.avg, top1.avg, outputs_new, targets_new)\n",
    "\n",
    "def save_checkpoint(state, is_best, checkpoint=args.out, filename='checkpoint.pth.tar'):\n",
    "    filepath = os.path.join(checkpoint, filename)\n",
    "    torch.save(state, filepath)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filepath, os.path.join(checkpoint, 'model_best.pth.tar'))\n",
    "\n",
    "def linear_rampup(current, rampup_length=args.epochs):\n",
    "    if rampup_length == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        current = np.clip(current / rampup_length, 0.0, 1.0)\n",
    "        return float(current)\n",
    "\n",
    "class WeightEMA(object):\n",
    "    def __init__(self, model, ema_model, alpha=0.999):\n",
    "        self.model = model\n",
    "        self.ema_model = ema_model\n",
    "        self.alpha = alpha\n",
    "        self.params = list(model.state_dict().values())\n",
    "        self.ema_params = list(ema_model.state_dict().values())\n",
    "        self.wd = 0.02 * args.lr\n",
    "\n",
    "        for param, ema_param in zip(self.params, self.ema_params):\n",
    "            param.data.copy_(ema_param.data)\n",
    "\n",
    "    def step(self):\n",
    "        one_minus_alpha = 1.0 - self.alpha\n",
    "        for param, ema_param in zip(self.params, self.ema_params):\n",
    "            if ema_param.dtype==torch.float32:\n",
    "                ema_param.mul_(self.alpha)\n",
    "                ema_param.add_(param * one_minus_alpha)\n",
    "                # customized weight decay\n",
    "                param.mul_(1 - self.wd)\n",
    "\n",
    "def mask_generate(max_probs, max_idx, batch, threshold):\n",
    "    mask_ori = torch.zeros(batch)\n",
    "    for i in range(7):\n",
    "        idx = np.where(max_idx.cpu() == i)[0]\n",
    "        m = max_probs[idx].ge(threshold[i]).float()\n",
    "        for k in range(len(idx)):\n",
    "            mask_ori[idx[k]]+=m[k]\n",
    "    return mask_ori.cuda()\n",
    "\n",
    "def adaptive_threshold_generate(outputs, targets, threshold, epoch):\n",
    "    outputs_l = outputs[1:, :]\n",
    "    targets_l = targets[1:]\n",
    "    probs = torch.softmax(outputs_l, dim=1)\n",
    "    max_probs, max_idx = torch.max(probs, dim=1)\n",
    "    eq_idx = np.where(targets_l.eq(max_idx).cpu() == 1)[0]\n",
    "\n",
    "    probs_new = max_probs[eq_idx]\n",
    "    targets_new = targets_l[eq_idx]\n",
    "    for i in range(7):\n",
    "        idx = np.where(targets_new.cpu() == i)[0]\n",
    "        if idx.shape[0] != 0:\n",
    "            threshold[i] = probs_new[idx].mean().cpu() * 0.97 / (1 + math.exp(-1 * epoch)) if probs_new[idx].mean().cpu() * 0.97 / (1 + math.exp(-1 * epoch)) >= 0.8 else 0.8\n",
    "        else:\n",
    "            threshold[i] = 0.8\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0f954f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4705d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
